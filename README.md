# Manufacturing & Financial Data Analytics Hub

## ğŸ“Š Project Overview
end-to-end data pipeline platform that integrates manufacturing operational data with financial metrics. Built with Python, PostgreSQL, and Apache Airflow, it demonstrates modern data engineering practices including ETL orchestration, star schema warehousing, and business intelligence visualization.

The project showcases the complete data lifecycle from raw data ingestion through transformation to interactive dashboardsâ€”providing a scalable foundation for manufacturing performance analysis.

---

## ğŸ¯ Project Goals

- Build a **production-ready ETL pipeline** for manufacturing data
- Implement a **star schema data warehouse** in PostgreSQL for analytical queries
- Demonstrate **workflow orchestration** with Apache Airflow
- Create **interactive BI dashboards** with Tableau Public
- Establish **CI/CD practices** using Docker and GitHub Actions
- Produce **actionable insights** combining manufacturing and financial metrics
---


## ğŸ§  How the Project Works

The system operates through three integrated layers:

### 1ï¸âƒ£ Data Ingestion & Staging
Raw manufacturing data (production logs, machine sensors) and financial records are:
- Extracted from source files/APIs
- Validated for completeness and accuracy
- Staged in temporary tables for transformation


### 2ï¸âƒ£ Transformation & Warehousing
The ETL process:
- Cleans and normalizes raw data
- Applies business logic and calculations
- Loads into a **star schema** with fact and dimension tables
- Maintains slowly changing dimensions for historical accuracy

### 3ï¸âƒ£ Analytics & Visualization
The curated data enables:
- Production efficiency tracking (OEE, downtime analysis)
- Cost per unit calculations
- Revenue and profitability trends
- Interactive Tableau dashboards for decision support

---

## ğŸ—ï¸ System Architecture Overview

At a high level, the system consists of:

- A **PostgreSQL database** with star schema design
- **Python ETL scripts** using Pandas and SQLAlchemy
- **Apache Airflow DAGs** for orchestration and scheduling
- **Tableau Public** for visualization and reporting
- **Docker containers** for consistent development/deployment
- **GitHub Actions** for automated testing and deployment


### Full System Architecture Diagram
![Architecture Diagram](docs/architecture.png)


## âŒ›ï¸ Runtime Sequence Explanation

The system follows this execution flow:

1. **Trigger** - Airflow DAG starts based on schedule or manual trigger
2. **Extract Phase** - Python scripts connect to data sources and pull raw data
3. **Staging** - Raw data is loaded into staging tables in PostgreSQL
4. **Transform Phase** - Data is cleaned, joined, and business logic is applied
5. **Load Phase** - Transformed data populates the star schema (fact/dimension tables)
6. **Validation** - Data quality checks ensure integrity and completeness
7. **Notification** - Success/failure alerts are logged and sent
8. **Visualization** - Tableau connects to the warehouse for dashboard updates

### Workflow States

The ETL pipeline transitions through these states:

- **ğŸŸ¡ Pending** - DAG initialized, waiting for execution
- **ğŸ”µ Running** - Tasks currently executing
- **ğŸŸ¢ Success** - All tasks completed successfully
- **ğŸ”´ Failed** - Error encountered, retry mechanism activated
- **ğŸ”„ Retrying** - Automatic retry of failed tasks
- **â¸ï¸ Paused** - Manual pause of DAG execution

---

## ğŸ› ï¸ Technology Stack

| Component          | Technology Choice                          |
|--------------------|---------------------------------------------|
| **Database**       | PostgreSQL 15+ (Star Schema Design)         |
| **ETL**            | Python 3.9+, Pandas, SQLAlchemy             |
| **Orchestration**  | Apache Airflow                              |
| **BI & Reporting** | Tableau Public                              |
| **Container**      | Docker, docker-compose                       |
| **CI/CD**          | GitHub Actions                              |
| **Version Control**| Git/GitHub                                  |
| **Monitoring**     | Airflow Logs, `etl_pipeline.log`              |

---


## ğŸ“ Project Structure

```manufacturing_analytics/
â”‚
â”œâ”€â”€ ğŸ“‚ src/ # Core ETL code
â”‚ â”œâ”€â”€ ğŸ“‚ extract/ # Data extraction modules
â”‚ â”‚ â”œâ”€â”€ extract_production.py
â”‚ â”‚ â”œâ”€â”€ extract_financial.py
â”‚ â”‚ â””â”€â”€ extract_machine_data.py
â”‚ â”‚
â”‚ â”œâ”€â”€ ğŸ“‚ transform/ # Data transformation logic
â”‚ â”‚ â”œâ”€â”€ clean_data.py
â”‚ â”‚ â”œâ”€â”€ calculate_kpis.py
â”‚ â”‚ â””â”€â”€ merge_datasets.py
â”‚ â”‚
â”‚ â””â”€â”€ ğŸ“‚ load/ # Database loading scripts
â”‚ â”œâ”€â”€ load_dimensions.py
â”‚ â””â”€â”€ load_facts.py
â”‚
â”œâ”€â”€ ğŸ“‚ airflow/
â”‚ â””â”€â”€ ğŸ“‚ dags/ # Airflow DAG definitions
â”‚ â”œâ”€â”€ manufacturing_etl.py # Main ETL pipeline DAG
â”‚ â””â”€â”€ data_quality_dag.py # Data validation DAG
â”‚
â”œâ”€â”€ ğŸ“‚ config/ # Configuration files
â”‚ â”œâ”€â”€ database.ini # DB connection settings
â”‚ â””â”€â”€ logging.conf # Logging configuration
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/ # Jupyter notebooks for exploration
â”‚ â””â”€â”€ exploratory_analysis.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ docs/ # Documentation
â”‚ â””â”€â”€ data_dictionary.md # Schema documentation
â”‚
â”œâ”€â”€ ğŸ“‚ tests/ # Unit and integration tests
â”‚ â”œâ”€â”€ test_extract.py
â”‚ â”œâ”€â”€ test_transform.py
â”‚ â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ ğŸ“‚ .vscode/ # VS Code configuration
â”‚ â””â”€â”€ settings.json
â”‚
â”œâ”€â”€ ğŸ“„ PostgreSQL_Schema.sql # Complete database schema
â”œâ”€â”€ ğŸ“„ DB_Manipulation_Queries.sql # Sample analytical queries
â”œâ”€â”€ ğŸ“„ docker-compose.yml # Container orchestration
â”œâ”€â”€ ğŸ“„ .env.example # Environment variables template
â”œâ”€â”€ ğŸ“„ requirements.txt # Python dependencies
â”œâ”€â”€ ğŸ“„ environment.yml # Conda environment
â”œâ”€â”€ ğŸ“„ start_postgres.py # DB initialization helper
â”œâ”€â”€ ğŸ“„ etl_pipeline.log # Pipeline execution logs
â”œâ”€â”€ ğŸ“„ .gitattributes # Git attributes
â”œâ”€â”€ ğŸ“„ .gitignore # Git ignore rules
â””â”€â”€ ğŸ“„ README.md # You are here Mate!!
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.9+
- PostgreSQL 15+
- Docker (optional, for containerized setup)
- Git

### Step 1: Clone the Repository

```bash
git clone https://github.com/IT21314742/manufacturing_analytics.git
cd manufacturing_analytics
```

### Step 2: Set Up Python Environment
Using pip:
```
pip install -r requirements.txt
```

Using Conda:
```
conda env create -f environment.yml
conda activate manufacturing-analytics
```

### Step 3: Configure Database
1. Create a PostgreSQL database:
   ```
   CREATE DATABASE manufacturing_db;
   ```
## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- PostgreSQL 15+
- Git

### Installation
1. Clone repository:
```bash
git clone https://github.com/yourusername/manufacturing-analytics.git
cd manufacturing-analytics

